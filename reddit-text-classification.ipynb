{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7304687,"sourceType":"datasetVersion","datasetId":4238152}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n# Assuming 'text' is the column containing strings and 'depressed' is the boolean column\ndf = pd.read_csv('/kaggle/input/text-dataset/dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:44:10.824727Z","iopub.execute_input":"2024-01-12T11:44:10.825154Z","iopub.status.idle":"2024-01-12T11:44:10.920109Z","shell.execute_reply.started":"2024-01-12T11:44:10.825121Z","shell.execute_reply":"2024-01-12T11:44:10.919157Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n\n\n# Data Preprocessing\ndef preprocess_text(text):\n    # Add any custom preprocessing steps here\n    return text.lower()  # Example: Convert text to lowercase\n\ndf['clean_text'] = df['clean_text'].apply(preprocess_text)\n\nimport re\n\ndf['clean_text'] = df['clean_text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n\n# Split the dataset\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    df['clean_text'], df['is_depression'], test_size=0.2, random_state=42\n)\n\n# Load pre-trained DistilBERT model and tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\n# Tokenize and encode the text data\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\n\n# Create PyTorch DataLoader\ntrain_dataset = TensorDataset(\n    torch.tensor(train_encodings['input_ids']),\n    torch.tensor(train_encodings['attention_mask']),\n    torch.tensor(train_labels.values)  # Convert to numpy array to avoid shape issues\n)\ntest_dataset = TensorDataset(\n    torch.tensor(test_encodings['input_ids']),\n    torch.tensor(test_encodings['attention_mask']),\n    torch.tensor(test_labels.values)  # Convert to numpy array to avoid shape issues\n)\n\n# Training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # Add this line\n\nnum_epochs = 50\nearly_stopping_patience = 5\nbest_validation_loss = float('inf')\nno_improvement_count = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n    \n    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n        inputs = {'input_ids': batch[0].to(device),\n                  'attention_mask': batch[1].to(device),\n                  'labels': batch[2].to(device)}\n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        # Calculate accuracy\n        logits = outputs.logits\n        predicted_labels = torch.argmax(logits, dim=1)\n        total_correct += (predicted_labels == inputs['labels']).sum().item()\n        total_samples += len(inputs['labels'])\n        \n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = total_loss / len(train_loader)\n    accuracy = total_correct / total_samples\n    \n    print(f'Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n    \n    # Validation\n    model.eval()\n    with torch.no_grad():\n        validation_loss = 0\n        validation_correct = 0\n        validation_samples = 0\n        \n        for batch in tqdm(test_loader, desc=f'Validation'):\n            inputs = {'input_ids': batch[0].to(device),\n                      'attention_mask': batch[1].to(device),\n                      'labels': batch[2].to(device)}\n            outputs = model(**inputs)\n            validation_loss += outputs.loss.item()\n            \n            # Calculate accuracy\n            logits = outputs.logits\n            predicted_labels = torch.argmax(logits, dim=1)\n            validation_correct += (predicted_labels == inputs['labels']).sum().item()\n            validation_samples += len(inputs['labels'])\n        \n        avg_validation_loss = validation_loss / len(test_loader)\n        validation_accuracy = validation_correct / validation_samples\n        \n        print(f'Validation Loss: {avg_validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}')\n        \n        # Early Stopping Check\n        if avg_validation_loss < best_validation_loss:\n            best_validation_loss = avg_validation_loss\n            no_improvement_count = 0\n        else:\n            no_improvement_count += 1\n            if no_improvement_count >= early_stopping_patience:\n                print(f'No improvement for {early_stopping_patience} epochs. Early stopping...')\n                break\n","metadata":{"execution":{"iopub.status.busy":"2024-01-12T11:59:59.794200Z","iopub.execute_input":"2024-01-12T11:59:59.794626Z","iopub.status.idle":"2024-01-12T12:21:30.736130Z","shell.execute_reply.started":"2024-01-12T11:59:59.794589Z","shell.execute_reply":"2024-01-12T12:21:30.735014Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Average Loss: 0.1136, Accuracy: 0.9571\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0933, Validation Accuracy: 0.9625\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50, Average Loss: 0.0427, Accuracy: 0.9845\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0673, Validation Accuracy: 0.9780\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50, Average Loss: 0.0218, Accuracy: 0.9939\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0700, Validation Accuracy: 0.9793\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50, Average Loss: 0.0111, Accuracy: 0.9971\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1118, Validation Accuracy: 0.9793\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50, Average Loss: 0.0060, Accuracy: 0.9982\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0850, Validation Accuracy: 0.9741\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50, Average Loss: 0.0030, Accuracy: 0.9994\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1105, Validation Accuracy: 0.9774\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 773/773 [02:47<00:00,  4.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50, Average Loss: 0.0119, Accuracy: 0.9974\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 194/194 [00:13<00:00, 14.69it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1431, Validation Accuracy: 0.9729\nNo improvement for 5 epochs. Early stopping...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def detect_depression(input_text):\n    input_encoding = tokenizer(input_text, return_tensors='pt')\n    input_encoding = {key: value.to(device) for key, value in input_encoding.items()}\n    output = model(**input_encoding)\n    probability = torch.sigmoid(output.logits)\n    probability_positive_class = probability[:, 1].item()  # Extract probability for class 1\n    prediction = 1 if probability_positive_class >=0.5 else 0\n    return prediction\n\n\n# Example usage\nnew_input_text = input(\"enter\")\nnew_prediction = detect_depression(new_input_text)\nprint(f\"Predicted class: {new_prediction}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-12T12:38:10.667881Z","iopub.execute_input":"2024-01-12T12:38:10.668901Z","iopub.status.idle":"2024-01-12T12:38:15.682113Z","shell.execute_reply.started":"2024-01-12T12:38:10.668861Z","shell.execute_reply":"2024-01-12T12:38:15.681180Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdin","text":"enter  feel very depressed.\n"},{"name":"stdout","text":"Predicted class: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'train_loss': avg_loss,\n    'train_accuracy': accuracy,\n    'validation_loss': avg_validation_loss,\n    'validation_accuracy': validation_accuracy\n}, 'distilbert_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-12T12:44:40.434508Z","iopub.execute_input":"2024-01-12T12:44:40.434932Z","iopub.status.idle":"2024-01-12T12:44:41.701210Z","shell.execute_reply.started":"2024-01-12T12:44:40.434899Z","shell.execute_reply":"2024-01-12T12:44:41.700394Z"},"trusted":true},"execution_count":37,"outputs":[]}]}